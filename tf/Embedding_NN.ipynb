{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final user selected: 733\n",
      "Final ratings selected: 28979\n"
     ]
    }
   ],
   "source": [
    "from helper.dataset_helper import get_ml_100k_dataset_simple\n",
    "from helper.dataset_helper import get_sampled_ml_100k_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final user selected: 892\n",
      "Final ratings selected: 46791\n"
     ]
    }
   ],
   "source": [
    "# A helper function to prepare the training data\n",
    "def get_training_data():\n",
    "    \"\"\"Get training data to be feed into Embedding-NN network.\n",
    "    \n",
    "    Due to the fact that we are using embeddings, the training data would\n",
    "    be a little different from what we have evern seen before. Each\n",
    "    training data would be n movie ids. [id1, id2, ..., idn].\n",
    "    \"\"\"\n",
    "    from helper.dataset_helper import get_sampled_ml_100k_data\n",
    "    \n",
    "    num_movies = 800\n",
    "    min_num_ratings = 10\n",
    "    num_neg = 50\n",
    "    \n",
    "    data, movie_indexer = get_sampled_ml_100k_data(\n",
    "        num_movies=num_movies,\n",
    "        min_num_ratings=min_num_ratings,\n",
    "        num_neg=num_neg\n",
    "    )\n",
    "    \n",
    "    num_users = len(data)\n",
    "    num_movie_id = 5\n",
    "    \n",
    "    training_data = np.zeros((num_users, num_movie_id))\n",
    "    training_labels = np.zeros((num_users, num_movies))\n",
    "    testing_data = []\n",
    "    for i in range(num_users):\n",
    "        training_data[i, :] = np.array(data[i][1][:num_movie_id])\n",
    "        testing_data.append((data[i][0], data[i][1][num_movie_id:], data[i][2]))\n",
    "        label = np.zeros(num_movies)\n",
    "        label[data[i][1][:num_movie_id]] = 1\n",
    "        training_labels[i, :] = label\n",
    "    return training_data, training_labels, testing_data\n",
    "\n",
    "training_data, training_labels, testing_data = get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 5)\n",
      "(714, 500)\n"
     ]
    }
   ],
   "source": [
    "print training_data.shape\n",
    "print training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_batch(training_data, training_labels, batch_size=20):\n",
    "    random_ids = np.random.choice(np.arange(training_data.shape[0]), batch_size, replace=False)\n",
    "    return training_data[random_ids, :], training_labels[random_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[ 3581.89819336  4085.59887695  6174.83203125 ..., -2070.44580078\n",
      "   2527.50073242   317.35189819]\n",
      " [ 4339.84277344  1314.72180176  4335.30224609 ..., -2849.47167969\n",
      "   2786.47363281  1166.46813965]\n",
      " [ 2509.38964844   199.05534363  3796.55297852 ..., -2327.47363281\n",
      "   2860.08911133   374.29153442]\n",
      " ..., \n",
      " [ 2635.72460938  -856.71337891  2433.45947266 ...,  -592.17401123\n",
      "   2783.60180664   766.546875  ]\n",
      " [ 2531.4765625   1285.96337891   503.64266968 ..., -1359.64868164\n",
      "    136.55072021 -1672.65246582]\n",
      " [ 2803.03637695  2634.57714844  2380.50439453 ...,  -541.58630371\n",
      "   1162.68566895  -698.96185303]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up the neural network, the overall workflow is:\n",
    "    \n",
    "    fetch_embeddings -> avg -> fc -> relu -> softmax loss\n",
    "\"\"\"\n",
    "embedding_dim = 128\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "num_movie_id = 5\n",
    "num_classes = 800\n",
    "batch_size = 20\n",
    "training_step = 100\n",
    "\n",
    "final_embeddings = np.zeros((training_data.shape[0], num_classes))\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, num_movie_id])\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size, num_classes])\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([num_classes, embedding_dim], -1.0, 1.0))\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, x)\n",
    "    reshaped_embed = tf.reshape(embed, (batch_size, embedding_dim * num_movie_id))\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([embedding_dim * num_movie_id, n_hidden_1]))\n",
    "    b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(reshaped_embed, W1), b1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "    b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W2), b2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # TODO: make sure this step is correct\n",
    "#     Wout = tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "#     bout = tf.Variable(tf.random_normal([num_classes]))\n",
    "#     out = tf.add(tf.matmul(layer_2, Wout), bout)\n",
    "    out = tf.matmul(layer_2, tf.transpose(embeddings))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for i in range(training_step):\n",
    "        batch_data, batch_labels = get_training_batch(training_data, training_labels, batch_size)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict={\n",
    "            x: batch_data,\n",
    "            y: batch_labels,\n",
    "        })\n",
    "    \n",
    "    for i in range(np.ceil(training_data.shape[0]/batch_size).astype(np.int32)):\n",
    "        batch_data = training_data[i * batch_size : (i+1) * batch_size, :]\n",
    "        batch_labels = training_labels[i * batch_size : (i+1) * batch_size, :]\n",
    "        \n",
    "        final_embeddings[i * batch_size : (i+1) * batch_size, :] = out.eval(feed_dict={x: batch_data, y: batch_labels})\n",
    "    \n",
    "    print final_embeddings[:10, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(892, 800)\n"
     ]
    }
   ],
   "source": [
    "print final_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
